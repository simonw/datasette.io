name: Deploy

on:
  push:
  workflow_dispatch:
    inputs:
      from_scratch:
        description: Enter 'skip' to create a new database from scratch
  schedule:
    - cron:  '26 4 * * *'

jobs:
  scheduled:
    runs-on: ubuntu-latest
    steps:
    - name: Check out repo
      uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v1
      with:
        python-version: 3.8
    - uses: actions/cache@v2
      name: Configure pip caching
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install Python dependencies
      run: |
        pip install -r requirements.txt
    - name: Run the pytest tests
      run: pytest
    - name: Download previous content.db
      if: github.event.inputs.from_scratch != 'skip'
      run: |
        curl -O https://datasette.io/content.db
    - name: Download previous blog.db
      if: github.event.inputs.from_scratch != 'skip'
      run: |
        curl --fail -o blog.db https://datasette.io/blog.db
      continue-on-error: true
    - name: Build databases
      run: scripts/build.sh
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    - name: Fetch documentation database for search index
      run: curl -o docs-index.db https://stable-docs.datasette.io/docs.db
    - name: Soundness check
      run: scripts/test.sh
    - name: Set up Cloud Run
      uses: google-github-actions/setup-gcloud@v0
      with:
        version: '275.0.0'
        service_account_email: ${{ secrets.GCP_SA_EMAIL }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
    - name: Deploy to Cloud Run
      run: scripts/deploy.sh
